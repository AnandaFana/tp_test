# torchrun --nproc_per_node=2 --master_port=31415 test_tp_gemini.py

import torch
import torch.distributed as dist
import torch.nn.functional as F
import numpy as np
import os
import math
import time
from typing import Dict, List, Optional, Callable

# å°è¯•å¯¼å…¥è¡¨æ ¼åº“ï¼Œå¦‚æœæ²¡æœ‰åˆ™é™çº§å¤„ç†
try:
    import pandas as pd
    from tabulate import tabulate
    HAS_PANDAS = True
except ImportError:
    HAS_PANDAS = False


# ================= åŸºç¡€å·¥å…·å‡½æ•° =================
import matplotlib.pyplot as plt
import numpy as np
import torch
import os
from scipy.stats import binned_statistic
import torch
import numpy as np
import matplotlib.pyplot as plt
import os
from scipy.stats import binned_statistic
from scipy.ndimage import gaussian_filter1d
from typing import Union


def plot_error_distribution(
    y_true: torch.Tensor,
    y_pred: torch.Tensor,
    title: str,
    save_dir: str = "tp_plots",
    smooth_sigma: float = 3.0,
    max_samples: int = 100000,
    n_bins: int = 50,
    min_valid_error: float = 1e-9,
    log_base: float = 10.0,
) -> None:
    """
    ç»˜åˆ¶è¾“å‡ºå¹…åº¦ vs æœ€å¤§ç›¸å¯¹è¯¯å·®çš„åŒ…ç»œå›¾ï¼ˆEnvelope Plotï¼‰

    å‚æ•°:
        y_true (torch.Tensor): çœŸå®æ ‡ç­¾ã€‚
        y_pred (torch.Tensor): é¢„æµ‹å€¼ã€‚
        title (str): å›¾è¡¨æ ‡é¢˜ã€‚
        save_dir (str): ä¿å­˜ç›®å½•ã€‚
        smooth_sigma (float): é«˜æ–¯å¹³æ»‘æ ‡å‡†å·®ã€‚
        max_samples (int): æ•£ç‚¹é‡‡æ ·æ•°ã€‚
        n_bins (int): åˆ†ç®±æ•°ã€‚
        min_valid_error (float): æœ‰æ•ˆè¯¯å·®ä¸‹é™ã€‚
        log_base (float): å¯¹æ•°åº•æ•°ã€‚
    """
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)

    # === 1. æ•°æ®å‡†å¤‡ ===
    if y_true.shape != y_pred.shape:
        raise ValueError(f"Shapes do not match: {y_true.shape} vs {y_pred.shape}")

    y_t = y_true.float().cpu().numpy().flatten()
    y_p = y_pred.float().cpu().numpy().flatten()

    x_val = np.abs(y_t) + 1e-12
    y_val = np.abs(y_t - y_p) / (np.abs(y_t) + 1e-9)

    # === 2. æ•£ç‚¹é‡‡æ · ===
    if len(x_val) > max_samples:
        idx = np.random.choice(len(x_val), max_samples, replace=False)
        x_scatter, y_scatter = x_val[idx], y_val[idx]
    else:
        x_scatter, y_scatter = x_val, y_val

    # === 3. è®¡ç®—æœ€å¤§åŒ…ç»œ ===
    min_exp = np.floor(np.log10(x_val.min()))
    max_exp = np.ceil(np.log10(x_val.max()))
    bins = np.logspace(min_exp, max_exp, n_bins)

    bin_maxs, bin_edges, _ = binned_statistic(x_val, y_val, statistic='max', bins=bins)
    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2

    # æ’é™¤ NaN å’Œ Inf
    valid_mask = (~np.isnan(bin_maxs)) & (~np.isinf(bin_maxs)) & (bin_maxs > 0)
    if not np.any(valid_mask):
        print(f"âš ï¸ æ— æœ‰æ•ˆè¯¯å·®æ•°æ®ï¼Œè·³è¿‡ç»˜å›¾: {title}")
        return

    x_valid = bin_centers[valid_mask]
    y_valid = bin_maxs[valid_mask]

    # é™åˆ¶æœ€å°æœ‰æ•ˆè¯¯å·®
    y_valid[y_valid < min_valid_error] = min_valid_error

    # === 4. å¹³æ»‘æ›²çº¿ ===
    y_smooth = gaussian_filter1d(y_valid, sigma=smooth_sigma)
    x_smooth = x_valid

    # === 5. ç»˜å›¾ ===
    plt.figure(figsize=(10, 6), dpi=150)

    # èƒŒæ™¯æ•£ç‚¹
    plt.scatter(x_scatter, y_scatter, s=3, alpha=0.15, color='#1f77b4', rasterized=True, label='Raw Errors')

    # åŒ…ç»œçº¿
    plt.plot(x_smooth, y_smooth, color='#D62728', linewidth=3, alpha=0.8, label='Max Error Envelope (Smoothed)')

    # è®¾ç½®åæ ‡è½´
    plt.xscale('log')
    plt.yscale('log')

    # æ ‡æ³¨
    plt.xlabel('Output Magnitude (|y|) - Log Scale', fontsize=12)
    plt.ylabel('Relative Error (Max) - Log Scale', fontsize=12)
    plt.title(f'{title}\nMaximum Error Envelope', fontsize=14)
    plt.grid(True, which="major", alpha=0.3)
    plt.legend(loc='upper right')

    # ä¿å­˜
    clean_title = title.replace(" ", "_").replace("(", "").replace(")", "").replace("@", "at")
    path = os.path.join(save_dir, f"{clean_title}_envelope.png")
    plt.savefig(path, bbox_inches='tight')
    plt.close()
    print(f"   ğŸ“ˆ åŒ…ç»œçº¿ç»˜å›¾å®Œæˆ: {path}")
# def plot_error_distribution(y_true, y_pred, title, save_dir="tp_plots"):
#     """
#     ç»˜åˆ¶ Output Magnitude vs Relative Error çš„æ•£ç‚¹è¶‹åŠ¿å›¾

#     å‚æ•°:
#         y_true: Baseline tensor (ç†è®ºçœŸå€¼)
#         y_pred: Comparison tensor (TPæ¨¡æ‹Ÿå€¼)
#         title: å›¾è¡¨æ ‡é¢˜
#         save_dir: ä¿å­˜ç›®å½•
#     """
#     if not os.path.exists(save_dir):
#         os.makedirs(save_dir)

#     # 1. æ•°æ®é¢„å¤„ç†
#     # è½¬ä¸º CPU numpyï¼Œå±•å¹³
#     y_t = y_true.detach().float().cpu().numpy().flatten()
#     y_p = y_pred.detach().float().cpu().numpy().flatten()

#     # è®¡ç®—ç»å¯¹å€¼å¤§å° (Xè½´) å’Œ ç›¸å¯¹è¯¯å·® (Yè½´)
#     # åŠ ä¸Šæå°å€¼ 1e-12 é˜²æ­¢ log(0) æŠ¥é”™
#     abs_values = np.abs(y_t) + 1e-12
#     abs_diff = np.abs(y_t - y_p)
#     rel_err = abs_diff / abs_values

#     # 2. é‡‡æ · (Downsample)
#     # 100ä¸‡ä¸ªç‚¹ç”»æ•£ç‚¹å›¾å¤ªæ…¢ï¼Œéšæœºé‡‡æ · 50,000 ä¸ªç‚¹å³å¯çœ‹æ¸…åˆ†å¸ƒ
#     n_samples = 100000
#     if len(y_t) > n_samples:
#         print('XXXXX   --  ', len(y_t))
#         indices = np.random.choice(len(y_t), n_samples, replace=False)
#         x_plot = abs_values[indices]
#         y_plot = rel_err[indices]
#     else:
#         x_plot = abs_values
#         y_plot = rel_err

#     # 3. è®¡ç®—è¶‹åŠ¿çº¿ (Binned Median)
#     # å°† X è½´æ•°æ®åœ¨ Log ç©ºé—´å‡åŒ€åˆ‡åˆ†ä¸º 50 ä¸ªæ¡¶ï¼Œçœ‹æ¯ä¸ªæ¡¶é‡Œçš„è¯¯å·®ä¸­ä½æ•°
#     # ä½¿ç”¨ Log space bins
#     min_exp = np.floor(np.log10(x_plot.min()))
#     max_exp = np.ceil(np.log10(x_plot.max()))
#     bins = np.logspace(min_exp, max_exp, 50)

#     bin_means, bin_edges, _ = binned_statistic(x_plot, y_plot, statistic='max', bins=bins)
#     bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2

#     # 4. å¼€å§‹ç»˜å›¾
#     plt.figure(figsize=(10, 6), dpi=150)

#     # A. ç»˜åˆ¶æ•£ç‚¹ (æµ…è“è‰²ï¼Œé€æ˜åº¦é«˜ï¼Œä½œä¸ºèƒŒæ™¯)
#     plt.scatter(x_plot, y_plot, alpha=0.3, s=3, color='#1f77b4', label='Individual Points', rasterized=True)

#     # B. ç»˜åˆ¶è¶‹åŠ¿çº¿ (çº¢è‰²ï¼ŒåŠ ç²—ï¼Œä½œä¸ºæ ¸å¿ƒç»“è®º)
#     plt.plot(bin_centers, bin_means, 'r-', linewidth=2.5, label='Maximum Error Trend')

#     # C. è®¾ç½®åæ ‡è½´ä¸ºå¯¹æ•°åæ ‡
#     plt.xscale('log')
#     plt.yscale('log')

#     # D. è£…é¥°
#     plt.xlabel('Output Magnitude (|y|) - Log Scale', fontsize=12)
#     plt.ylabel('Relative Error - Log Scale', fontsize=12)
#     plt.title(f'{title}\nError vs Magnitude Analysis', fontsize=14)
#     plt.grid(True, which="both", ls="-", alpha=0.2)
#     plt.legend(loc='upper right')

#     # E. æ ‡æ³¨ç²¾åº¦å‚è€ƒçº¿ (å¯é€‰)
#     # plt.axhline(y=1e-3, color='g', linestyle='--', alpha=0.5, label='FP16 Precision (~1e-3)')

#     # ä¿å­˜
#     clean_title = title.replace(" ", "_").replace("(", "").replace(")", "").replace("@", "at")
#     save_path = os.path.join(save_dir, f"{clean_title}_dist.png")
#     plt.savefig(save_path, bbox_inches='tight')
#     plt.close()
#     print(f"   ğŸ“ˆ ç»˜å›¾å®Œæˆ: {save_path}")

def setup_distributed():
    """åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ"""
    if 'RANK' not in os.environ:
        # æœ¬åœ°è°ƒè¯•ç”¨é»˜è®¤å€¼
        os.environ['RANK'] = '0'
        os.environ['LOCAL_RANK'] = '0'
        os.environ['WORLD_SIZE'] = '1'
        print("âš ï¸ æœªæ£€æµ‹åˆ°åˆ†å¸ƒå¼ç¯å¢ƒï¼Œä½¿ç”¨å•æœºæ¨¡æ‹Ÿæ¨¡å¼ (World Size=1)")

    rank = int(os.environ['RANK'])
    local_rank = int(os.environ['LOCAL_RANK'])
    world_size = int(os.environ['WORLD_SIZE'])

    if not dist.is_initialized():
        dist.init_process_group(backend='nccl', init_method='env://')

    torch.cuda.set_device(local_rank)
    return rank, local_rank, world_size

def set_deterministic(seed=42):
    """å¼ºåˆ¶ç¡®å®šæ€§ï¼Œç¡®ä¿ Baseline å’Œ TP çš„è¾“å…¥åˆå§‹åŒ–å®Œå…¨ä¸€è‡´"""
    os.environ["CUBLAS_WORKSPACE_CONFIG"] = ":4096:8"
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    torch.use_deterministic_algorithms(True, warn_only=True)
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True

# ================= æ ¸å¿ƒæµ‹è¯•æ¡†æ¶ç±» =================

class TPBenchmarkRunner:
    def __init__(self, save_dir="tp_artifacts"):
        self.rank, self.local_rank, self.world_size = setup_distributed()
        self.device = torch.device(f"cuda:{self.local_rank}")
        self.results_log = [] # å­˜å‚¨æ‰€æœ‰æµ‹è¯•ç»“æœ
        self.save_dir = save_dir

        if self.rank == 0:
            os.makedirs(self.save_dir, exist_ok=True)
            print(f"\n{'='*60}")
            print(f"ğŸš€ TP Benchmark Runner Initialized")
            print(f"   World Size: {self.world_size} | Device: {self.device}")
            print(f"   Artifacts Dir: {self.save_dir}")
            print(f"{'='*60}\n")

    def _reset_seeds(self, seed=42):
        """æ¯æ¬¡æµ‹è¯•å‰é‡ç½®ç§å­"""
        set_deterministic(seed)

    def _record_result(self, test_name, dtype, stage, abs_diff,abs_diff_mean, rel_diff, rel_diff_mean, output_tensor=None):
        """è®°å½•å•æ¬¡æµ‹è¯•ç»“æœ"""
        self.results_log.append({
            "Test Name": test_name,
            "Dtype": str(dtype).split('.')[-1],
            "Comparison": stage,
            "Max Abs Diff": abs_diff,
            "Mean Abs Diff": abs_diff_mean,
            "Max Rel Diff": rel_diff,
            "Mean Rel Diff": rel_diff_mean
        })

        # ä¿å­˜ Tensor ç”¨äºæº¯æº (å¯é€‰ï¼Œä»…åœ¨ Rank 0)
        if self.rank == 0 and output_tensor is not None:
            fname = f"{test_name}_{str(dtype).split('.')[-1]}_{stage.replace(' ', '_')}.pt"
            path = os.path.join(self.save_dir, fname)
            # torch.save(output_tensor.cpu(), path)

    def _compare_tensors(self, name, dtype, tensor_a, tensor_b, tag_a, tag_b):
        """å¯¹æ¯”ä¸¤ä¸ª Tensor å¹¶è¿”å›è¯¯å·®"""
        if self.rank != 0: return

        # ç»Ÿä¸€è½¬ float32 æ¯”è¾ƒä»¥å…æº¢å‡º
        a_f32 = tensor_a.float()
        b_f32 = tensor_b.float()

        abs_diff = (a_f32 - b_f32).abs().max().item()
        abs_diff_mean = (a_f32 - b_f32).abs().mean().item()
        # ç›¸å¯¹è¯¯å·®å¤„ç†é™¤é›¶
        denominator = b_f32.abs() + 1e-8
        rel_diff = ((a_f32 - b_f32).abs() / denominator).max().item()
        rel_diff_mean = ((a_f32 - b_f32).abs() / denominator).mean().item()

        # === æ–°å¢ï¼šåªæœ‰å½“æœ‰æ˜¾è‘—è¯¯å·®æ—¶ï¼Œæ‰ç”»å›¾åˆ†æ ===
        # ä¾‹å¦‚ï¼šå¦‚æœæ˜¯ Simulated vs Real TP (å…¨æ˜¯0)ï¼Œå°±æ²¡å¿…è¦ç”»äº†ï¼Œæµªè´¹æ—¶é—´
        # ä½† Baseline vs Simulated (æœ‰æ•°å­¦è¯¯å·®) ä¸€å®šè¦ç”»
        if "Baseline" in tag_a and abs_diff > 1e-6:
            plot_title = f"{name} [{str(dtype).split('.')[-1]}]"
            # è°ƒç”¨ç»˜å›¾å‡½æ•°
            plot_error_distribution(a_f32, b_f32, plot_title, self.save_dir)
        # ==========================================

        self._record_result(name, dtype, f"{tag_a} vs {tag_b}", abs_diff,abs_diff_mean, rel_diff, rel_diff_mean, tensor_b)
        return abs_diff

    # ================= æµ‹è¯•åœºæ™¯ 1: Linear (Column Parallel) =================
    # å¯¹åº”ä½ åŸæ¥çš„ test_tp_linear
    # åœºæ™¯: è¾“å‡ºç»´åº¦è¢«åˆ‡åˆ†ï¼Œç»“æœéœ€è¦ All-Gather æ‹¼æ¥

    def run_case_linear_col_parallel(self, batch_size=4, in_feat=4096, out_feat=4096, dtype=torch.float16):
        test_name = "Linear(ColParallel)"
        if self.rank == 0: print(f"RUNNING: {test_name} [{dtype}]...")
        self._reset_seeds()

        # 1. å‡†å¤‡å…¨å±€æ•°æ® (Rank 0 ç”Ÿæˆï¼Œå¹¿æ’­)
        if self.rank == 0:
            x = torch.randn(batch_size, in_feat, device=self.device, dtype=torch.float32)
            w = torch.randn(out_feat, in_feat, device=self.device, dtype=torch.float32)
            b = torch.randn(out_feat, device=self.device, dtype=torch.float32)
        else:
            x = torch.zeros(batch_size, in_feat, device=self.device, dtype=torch.float32)
            w = torch.zeros(out_feat, in_feat, device=self.device, dtype=torch.float32)
            b = torch.zeros(out_feat, device=self.device, dtype=torch.float32)

        dist.broadcast(x, 0); dist.broadcast(w, 0); dist.broadcast(b, 0)
        x, w, b = x.to(dtype), w.to(dtype), b.to(dtype)

        # --- A. Baseline (å•å¡æ ‡å‡†) ---
        res_baseline = None
        if self.rank == 0:
            res_baseline = F.linear(x, w, b)

        # --- B. Simulation (å•å¡æ¨¡æ‹Ÿåˆ‡åˆ†) ---
        res_sim = None
        if self.rank == 0:
            # æ¨¡æ‹Ÿ Column Parallel: åˆ‡åˆ† Weight çš„ output dim (dim 0), Bias ä¹Ÿè¦åˆ‡
            w_chunks = w.chunk(self.world_size, dim=0)
            b_chunks = b.chunk(self.world_size, dim=0)

            sim_outputs = []
            for i in range(self.world_size):
                # æ¨¡æ‹Ÿä¸åŒ Rank çš„è®¡ç®—
                sim_outputs.append(F.linear(x, w_chunks[i], b_chunks[i]))

            # Column Parallel çš„åˆå¹¶æ–¹å¼æ˜¯ Concat
            res_sim = torch.cat(sim_outputs, dim=-1)

        # --- C. Real TP (çœŸå®åˆ†å¸ƒå¼) ---
        # 1. åˆ‡åˆ†æ•°æ®
        w_local_chunks = w.chunk(self.world_size, dim=0)
        b_local_chunks = b.chunk(self.world_size, dim=0)
        w_local = w_local_chunks[self.rank]
        b_local = b_local_chunks[self.rank]

        # 2. æœ¬åœ°è®¡ç®—
        y_local = F.linear(x, w_local, b_local)

        # 3. é€šä¿¡ (All-Gather)
        y_gathered_list = [torch.zeros_like(y_local) for _ in range(self.world_size)]
        dist.all_gather(y_gathered_list, y_local)
        res_real = torch.cat(y_gathered_list, dim=-1)

        # --- éªŒè¯ ---
        if self.rank == 0:
            self._compare_tensors(test_name, dtype, res_baseline, res_sim, "Baseline(TP1)", "Simulated")
            self._compare_tensors(test_name, dtype, res_sim, res_real, "Simulated", "Real TP")

    # ================= æµ‹è¯•åœºæ™¯ 2: MatMul (Row Parallel Sum) =================
    # å¯¹åº”ä½ åŸæ¥çš„ test_tp_matmul_precision
    # åœºæ™¯: è¾“å…¥ç»´åº¦è¢«åˆ‡åˆ† (Kç»´)ï¼Œç»“æœéœ€è¦ All-Reduce (Sum)

    def run_case_matmul_row_parallel(self, N=1024, mean = 0., dtype=torch.float16):
        test_name = f"MatMul(A@B), Mean = {mean}"
        if self.rank == 0: print(f"RUNNING: {test_name} [{dtype}]...")
        self._reset_seeds()

        # Data Generation
        if self.rank == 0:
            A = torch.randn(N, N, device=self.device).to(dtype) + mean
            B = torch.randn(N, N, device=self.device).to(dtype)
        else:
            A = torch.zeros(N, N, device=self.device, dtype=dtype)
            B = torch.zeros(N, N, device=self.device, dtype=dtype)
        dist.broadcast(A, 0); dist.broadcast(B, 0)

        # --- A. Baseline ---
        res_base = None
        if self.rank == 0:
            res_base = torch.mm(A, B)

        # --- B. Simulation ---
        res_sim = None
        if self.rank == 0:
            # Row Parallel: A æŒ‰åˆ—åˆ‡ (dim 1), B æŒ‰è¡Œåˆ‡ (dim 0) -> ç»“æœ Sum
            A_chunks = A.chunk(self.world_size, dim=1)
            B_chunks = B.chunk(self.world_size, dim=0)

            partials = []
            for i in range(self.world_size):
                partials.append(torch.mm(A_chunks[i], B_chunks[i]))
            res_sim = sum(partials) # æ¨¡æ‹Ÿ Reduce Sum

        # --- C. Real TP ---
        # å‡†å¤‡åˆ†ç‰‡
        A_local = A.chunk(self.world_size, dim=1)[self.rank]
        B_local = B.chunk(self.world_size, dim=0)[self.rank]

        # è®¡ç®—ä¸é€šä¿¡
        y_local = torch.mm(A_local, B_local)
        dist.all_reduce(y_local, op=dist.ReduceOp.SUM)
        res_real = y_local

        # --- éªŒè¯ ---
        if self.rank == 0:
            self._compare_tensors(test_name, dtype, res_base, res_sim, "Baseline(TP1)", "Simulated")
            self._compare_tensors(test_name, dtype, res_sim, res_real, "Simulated", "Real TP")

    # ================= æµ‹è¯•åœºæ™¯ 3: MLP Chain (A@B@C) =================
    # å¯¹åº”ä½ åŸæ¥çš„ test_llm_style_tp_3matmul (MLP Style)
    # æµç¨‹: X -> [Col Split] -> Y_mid -> [Row Split] -> Y_out -> AllReduce

    def run_case_mlp_chain(self, size=1024, use_relu=False, mean = 0., dtype=torch.bfloat16):
        test_name = f"MLP( sigma(x@A)@B ), Mean = {mean}"
        if self.rank == 0: print(f"RUNNING: {test_name} [{dtype}]...")
        self._reset_seeds()

        # X, A(Up_proj), B(Down_proj)
        # å‡è®¾ A æ˜¯ expand (1024->4096), B æ˜¯ shrink (4096->1024)
        # ä¸ºäº†ç®€åŒ–å’Œä½ ä¹‹å‰çš„ä¾‹å­ä¸€è‡´ï¼Œæˆ‘ä»¬ç”¨ size x size
        hidden_size = size
        inter_size = size

        if self.rank == 0:
            X = torch.randn(1, hidden_size, device=self.device).to(dtype) + mean
            W_up = torch.randn(hidden_size, inter_size, device=self.device).to(dtype) * math.sqrt(2.0 / hidden_size) # A
            W_down = torch.randn(inter_size, hidden_size, device=self.device).to(dtype) * math.sqrt(2.0 / hidden_size)# B
        else:
            X = torch.zeros(1, hidden_size, device=self.device, dtype=dtype)
            W_up = torch.zeros(hidden_size, inter_size, device=self.device, dtype=dtype)
            W_down = torch.zeros(inter_size, hidden_size, device=self.device, dtype=dtype)

        dist.broadcast(X, 0); dist.broadcast(W_up, 0); dist.broadcast(W_down, 0)

        # --- A. Baseline ---
        res_base = None
        if self.rank == 0:
            # X @ A @ B
            mid = torch.mm(X, W_up)
            if use_relu:
                mid = F.relu(mid)
            res_base = torch.mm(mid, W_down)

        # --- B. Simulation ---
        res_sim = None
        if self.rank == 0:
            # 1. Col Parallel (W_up): WæŒ‰åˆ—åˆ‡
            W_up_chunks = W_up.chunk(self.world_size, dim=1)
            # 2. Row Parallel (W_down): WæŒ‰è¡Œåˆ‡
            W_down_chunks = W_down.chunk(self.world_size, dim=0)

            partials = []
            for i in range(self.world_size):
                # Local Path: X @ W_up_i -> W_down_i
                mid_i = torch.mm(X, W_up_chunks[i])
                if use_relu:
                    mid_i = F.relu(mid_i) # æ¿€æ´»å‡½æ•°æœ¬åœ°åš
                out_i = torch.mm(mid_i, W_down_chunks[i])
                partials.append(out_i)

            res_sim = sum(partials) # AllReduce Sum

        # --- C. Real TP ---
        # å‡†å¤‡åˆ†ç‰‡
        W_up_local = W_up.chunk(self.world_size, dim=1)[self.rank]
        W_down_local = W_down.chunk(self.world_size, dim=0)[self.rank]

        # æœ¬åœ°è®¡ç®—
        mid_local = torch.mm(X, W_up_local)
        if use_relu:
            mid_local = F.relu(mid_local)
        out_local = torch.mm(mid_local, W_down_local)

        # é€šä¿¡
        dist.all_reduce(out_local, op=dist.ReduceOp.SUM)
        res_real = out_local

        # --- éªŒè¯ ---
        if self.rank == 0:
            self._compare_tensors(test_name, dtype, res_base, res_sim, "Baseline(TP1)", "Simulated")
            self._compare_tensors(test_name, dtype, res_sim, res_real, "Simulated", "Real TP")

    # ================= æŠ¥å‘Šç”Ÿæˆ =================

    def generate_report(self):
        if self.rank != 0: return

        print(f"\n{'='*30} TEST REPORT {'='*30}")

        if HAS_PANDAS:
            df = pd.DataFrame(self.results_log)
            # æ ¼å¼åŒ–ä¸€ä¸‹æ•°å­—æ˜¾ç¤º
            print(tabulate(df, headers='keys', tablefmt='psql', floatfmt=".2e", showindex=False))

            # ä¿å­˜ CSV
            csv_path = os.path.join(self.save_dir, "tp_benchmark_summary.csv")
            df.to_csv(csv_path, index=False)
            print(f"\nğŸ“Š è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜è‡³: {csv_path}")
            print(f"ğŸ’¾ ä¸­é—´ Tensor å·²ä¿å­˜è‡³: {self.save_dir}/")
        else:
            # ç®€å•çš„ Fallback æ‰“å°
            print(f"{'Test':<20} | {'Dtype':<6} | {'Compare':<20} | {'Max Abs Err':<12} | {'Mean Abs Err':<12} | {'Max Rel Err':<12} {'Mean Rel Err':<12} |")
            print("-" * 80)
            for res in self.results_log:
                print(f"{res['Test Name']:<20} | {res['Dtype']:<6} | {res['Comparison']:<20} | {res['Max Abs Diff']:.2e}     |  {res['Mean Abs Diff']:.2e}     |   {res['Max Rel Diff']:.2e}  | {res['Mean Rel Diff']:.2e}     |")

# ================= ä¸»ç¨‹åºå…¥å£ =================

def main():
    runner = TPBenchmarkRunner()

    # 1. è¿è¡Œ Linear æµ‹è¯• (Col Parallel)
    for dtype in [torch.float32, torch.float16, torch.bfloat16]:
        runner.run_case_linear_col_parallel(dtype=dtype)

    # 2. è¿è¡Œ MatMul æµ‹è¯• (Row Parallel)
    for dtype in [torch.float32, torch.float16, torch.bfloat16]:
        runner.run_case_matmul_row_parallel(dtype=dtype)
        runner.run_case_matmul_row_parallel(mean= 1., dtype=dtype)



    # 3. è¿è¡Œ MLP Chain æµ‹è¯•
    for dtype in [torch.float32, torch.float16, torch.bfloat16]:
        runner.run_case_mlp_chain(use_relu=True, mean=0., dtype=dtype)
        runner.run_case_mlp_chain(use_relu=True, mean= .1, dtype=dtype)




    # 4. ç”Ÿæˆæ±‡æ€»è¡¨æ ¼
    runner.generate_report()

    dist.barrier()
    dist.destroy_process_group()

if __name__ == "__main__":
    main()